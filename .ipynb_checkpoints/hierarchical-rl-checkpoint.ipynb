{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=gpu1'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import sys\n",
    "sys.setrecursionlimit(10**6)\n",
    "experiment_setup_name = \"tutorial.gym.atari.hierarchical.v0\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'BankHeist-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 5\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 50\n",
    "\n",
    "\n",
    "#theano device selection. GPU is, as always, in preference, but not required\n",
    "%env THEANO_FLAGS='device=gpu1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-06-07 11:38:52,721] Making new env: BankHeist-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x134574450>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL4AAAEACAYAAAANw8wsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njkSFrziS/wX9ZX9N3zdymQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x134b94d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Parameters\n",
    "* observation dimensions, actions, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "n_actions = atari.action_space.n\n",
    "observation_shape = (None,)+atari.observation_space.shape\n",
    "action_names = atari.get_action_meanings()\n",
    "print(action_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from controller_class import Controller\n",
    "controller = Controller(observation_shape,n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from metacontroller_class import MetaController\n",
    "metacontroller = MetaController(controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.layers import get_layer_dtype\n",
    "\n",
    "#a nice pythonic interface\n",
    "def step(env_observation, \n",
    "         prev_memories = 'zeros',\n",
    "         batch_size = N_PARALLEL_GAMES,):\n",
    "    \"\"\" returns actions and new states given observation and prev state\n",
    "    Prev state in default setup should be [prev window,]\"\"\"\n",
    "    \n",
    "    if prev_memories == 'zeros':\n",
    "        controller_mem = metacontroller_mem = 'zeros'\n",
    "        meta_inp = np.zeros((batch_size,)+tuple(metacontroller.observation_shape[1:]),dtype='float32')\n",
    "        itr = -1\n",
    "        #goal will be defined by \"if itr ==0\" clause\n",
    "    else:\n",
    "        pivot = len(controller.agent.state_variables)    \n",
    "        controller_mem, metacontroller_mem = prev_memories[:pivot],prev_memories[pivot:-4]\n",
    "        meta_inp, goal, meta_V, itrs = prev_memories[-4:]\n",
    "        itr = itrs[0]\n",
    "        \n",
    "    itr = (itr+1)%metacontroller.period\n",
    "    \n",
    "    if itr==0:\n",
    "        goal,metacontroller_mem,meta_V = metacontroller.step(meta_inp,metacontroller_mem,batch_size)\n",
    "\n",
    "    \n",
    "    action,controller_mem,meta_inp = controller.step(env_observation,goal,controller_mem,batch_size)\n",
    "    \n",
    "    \n",
    "    new_memories = controller_mem + metacontroller_mem + [meta_inp, goal,meta_V, [itr]*N_PARALLEL_GAMES ]\n",
    "    \n",
    "    return action, new_memories\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* We define a small container that stores\n",
    " * game emulators\n",
    " * last agent observations\n",
    " * agent memories at last time tick\n",
    "* This allows us to instantly continue a session from where it stopped\n",
    "\n",
    "\n",
    "\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-06-07 14:21:58,623] Making new env: BankHeist-v0\n",
      "[2016-06-07 14:21:58,647] Making new env: BankHeist-v0\n",
      "[2016-06-07 14:21:58,670] Making new env: BankHeist-v0\n",
      "[2016-06-07 14:21:58,692] Making new env: BankHeist-v0\n",
      "[2016-06-07 14:21:58,715] Making new env: BankHeist-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import GamePool\n",
    "\n",
    "pool = GamePool(GAME_TITLE, N_PARALLEL_GAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DOWN' 'UPFIRE' 'UPFIRE' 'UPFIRE' 'UPFIRE']\n",
      " ['UPFIRE' 'FIRE' 'UPFIRE' 'DOWNRIGHTFIRE' 'RIGHT']\n",
      " ['UPFIRE' 'UPFIRE' 'UPFIRE' 'UP' 'UPFIRE']]\n",
      "CPU times: user 3.78 s, sys: 536 ms, total: 4.32 s\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "observation_log,action_log,reward_log,_,_,_  = pool.interact(step,50,verbose=True)\n",
    "\n",
    "print(np.array(action_names)[np.array(action_log)[:3,:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay pool\n",
    "\n",
    "Since our network exists in a theano graph and OpenAI gym doesn't, we shall train out network via experience replay.\n",
    "\n",
    "To do that in AgentNet, one can use a SessionPoolEnvironment.\n",
    "\n",
    "It's simple: you record new sessions using `interact(...)`, and than immediately train on them.\n",
    "\n",
    "1. Interact with Atari, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "controller_env = SessionPoolEnvironment(\n",
    "    observations = controller.agent.observation_layers,\n",
    "    actions=controller.resolver,\n",
    "    agent_memories=controller.agent.agent_states)\n",
    "\n",
    "\n",
    "metacontroller_env = SessionPoolEnvironment(\n",
    "    observations = metacontroller.agent.observation_layers,\n",
    "    actions=metacontroller.resolver,\n",
    "    agent_memories=metacontroller.agent.agent_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_envs(controller_env,metacontroller_env,pool,\n",
    "                n_steps=100,split_into = 1):\n",
    "    \"\"\" a function that creates new sessions and ads them into the pool\n",
    "    throwing the old ones away entirely for simplicity\"\"\"\n",
    "    assert n_steps%split_into == 0\n",
    "\n",
    "    \n",
    "    #get interaction sessions\n",
    "    observation_log,action_tensor,extrinsic_reward_log,memory_log,is_alive_tensor,_= pool.interact(step,n_steps=n_steps)\n",
    "    batch_size = observation_log.shape[0]\n",
    "    \n",
    "    #parse memory seq\n",
    "    #UNUSED pivot = len(controller.agent.state_variables)\n",
    "    #UNUSED controller_mem_log, metacontroller_mem_log = memory_log[:pivot],memory_log[pivot:-4]\n",
    "    meta_obs_log, goal_log,meta_V, itrs = memory_log[-4:]\n",
    "    itr = itrs[0]\n",
    "    \n",
    "    #parse pre-experience memory states \n",
    "    preceding_memory_states = list(pool.prev_memory_states)\n",
    "    pivot = len(controller.agent.state_variables)\n",
    "    controller_preceding_states = preceding_memory_states[:pivot]\n",
    "    metacontroller_preceding_states = preceding_memory_states[pivot:-4]\n",
    "\n",
    "    ###CONTROLLER###\n",
    "    #load them into experience replay environment for controller\n",
    "    \n",
    "    #controller_preceding_states =!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    ctrl_shape = (batch_size*split_into,n_steps/split_into)\n",
    "    \n",
    "    intrinsic_rewards = np.concatenate([np.zeros([meta_V.shape[0],1]), np.diff(meta_V,axis=1)],axis=1)\n",
    "    controller_env.load_sessions([observation_log.reshape(ctrl_shape+observation_shape[1:]),\n",
    "                                  goal_log.reshape(ctrl_shape)],\n",
    "                      action_tensor.reshape(ctrl_shape),\n",
    "                      intrinsic_rewards.reshape(ctrl_shape),\n",
    "                      is_alive_tensor.reshape(ctrl_shape),\n",
    "                      #controller_preceding_states\n",
    "                                )\n",
    "    \n",
    "    \n",
    "    ###METACONTROLLER###\n",
    "    #separate case for metacontroller\n",
    "    extrinsic_reward_sums = extrinsic_reward_log.reshape([batch_size,-1,metacontroller.period]).sum(axis=-1)\n",
    "    metacontroller_env.load_sessions(meta_obs_log[:,itr==0],\n",
    "                      goal_log[:,itr==0],\n",
    "                      extrinsic_reward_sums,\n",
    "                      is_alive_tensor[:,itr==0],\n",
    "                      metacontroller_preceding_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first  sessions\n",
    "update_envs(controller_env,metacontroller_env,pool,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated way of training is to store a large pool of sessions and train on random batches of them. \n",
    "* Why that is expected to be better - http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\n",
    "* Or less proprietary - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "To do that, one might make use of\n",
    "* ```env.load_sessions(...)``` - load new sessions\n",
    "* ```env.get_session_updates(...)``` - does the same thing via theano updates (advanced)\n",
    "* ```batch_env = env.sample_session_batch(batch_size, ...)``` - create an experience replay environment that contains batch_size random sessions from env (rerolled each time). Should be used in training instead of env.\n",
    "* ```env.select_session_batch(indices)``` does the same thing deterministically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.regularization import regularize_network_params, l2\n",
    "from agentnet.learning import qlearning_n_step\n",
    "\n",
    "def get_loss(env,agent,replay_seq_len):\n",
    "    #get agent's Qvalues obtained via experience replay\n",
    "    _,_,_,_,qvalues_seq = agent.get_sessions(\n",
    "        env,\n",
    "        #initial_hidden = env.preceding_agent_memories,\n",
    "        session_length=replay_seq_len,\n",
    "        batch_size=env.batch_size,\n",
    "        optimize_experience_replay=True,\n",
    "    )\n",
    "\n",
    "    scaled_reward_seq = env.rewards\n",
    "\n",
    "    elwise_mse_loss = qlearning_n_step.get_elementwise_objective(qvalues_seq,\n",
    "                                                            env.actions[0],\n",
    "                                                            scaled_reward_seq,\n",
    "                                                            env.is_alive,\n",
    "                                                            gamma_or_gammas=0.99,\n",
    "                                                            n_steps=5)\n",
    "\n",
    "    #compute mean over \"alive\" fragments\n",
    "    mse_loss = elwise_mse_loss.sum() / env.is_alive.sum()\n",
    "\n",
    "\n",
    "    #regularize network weights\n",
    "\n",
    "    reg_l2 = regularize_network_params(agent.state_variables.keys(),l2)*10**-5\n",
    "\n",
    "\n",
    "    return mse_loss + reg_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = get_loss(controller_env,controller.agent,50) + get_loss(metacontroller_env,metacontroller.agent,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this part we are using some basic Reinforcement Learning methods (here - Q-learning) to train\n",
    "* AgentNet has plenty of such methods, but we shall use the simple Q_learning for now.\n",
    "* Later you can try:\n",
    " * SARSA - simpler on-policy algorithms\n",
    " * N-step q-learning (requires n_steps parameter)\n",
    " * Advantage Actor-Critic (requires state values and probabilities instead of Q-values)\n",
    "\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - squared error against reference Q-values) values at each batch and tick\n",
    "  \n",
    "* If you want to do it the hard way instead, try .get_reference_Qvalues and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = controller.weights+metacontroller.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "import lasagne\n",
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mean session reward\n",
    "mean_session_reward = metacontroller_env.rewards.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "evaluation_fun = theano.function([],[loss,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session visualization tools\n",
    "\n",
    "Just a helper function that draws current game images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_sessions(max_n_sessions = 3):\n",
    "    \"\"\"just draw random images\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=[15,8])\n",
    "    \n",
    "    pictures = [atari.render(\"rgb_array\") for atari in pool.games[:max_n_sessions]]\n",
    "    for i,pic in enumerate(pictures):\n",
    "        plt.subplot(1,max_n_sessions,i+1)\n",
    "        plt.imshow(pic)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "display_sessions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils import save,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load(resolver,\"/srv/hd7/jheuristic/agentnet_snapshots/heist.epoch7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resolver = controller.resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 25000\n",
    "#25k may take hours to train.\n",
    "#consider interrupt early.\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_envs(controller_env,metacontroller_env,pool,replay_seq_len)\n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%1 ==0:\n",
    "        current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/1000.)\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%5 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        update_envs(controller_env,metacontroller_env,pool,replay_seq_len)\n",
    "        \n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "        update_envs(controller_env,metacontroller_env,pool,replay_seq_len)\n",
    "        \n",
    "        print(\"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy))\n",
    "        \n",
    "    if epoch_counter %500 ==0:\n",
    "        print(\"Learning curves:\")\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print(\"Random session examples\")\n",
    "        display_sessions()\n",
    "    \n",
    "    if epoch_counter %1000 ==0:\n",
    "        save(resolver,\"/srv/hd7/jheuristic/agentnet_snapshots/{}.epoch{}\".format(\"heist\",epoch_counter))\n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Random session examples\")\n",
    "display_sessions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission\n",
    "Here we simply run the OpenAI gym submission code and view scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resolver.epsilon.set_value(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[m.close() for m in gym.monitoring._open_monitors()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = '/tmp/AgentNet-simplenet-SpaceInvadersv0-Recording0'\n",
    "\n",
    "subm_env = gym.make(GAME_TITLE)\n",
    "\n",
    "#starting monitor. This setup does not write videos\n",
    "#subm_env.monitor.start(save_path,lambda i: False,force=True)\n",
    "\n",
    "#this setup does\n",
    "subm_env.monitor.start(save_path,force=True)\n",
    "\n",
    "\n",
    "for i_episode in xrange(10):\n",
    "    \n",
    "    #initial observation\n",
    "    observation = subm_env.reset()\n",
    "    #initial memory\n",
    "    prev_memories = \"zeros\"\n",
    "    \n",
    "    \n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        action,new_memories = step([observation],prev_memories,batch_size=1)\n",
    "        observation, reward, done, info = subm_env.step(action[0])\n",
    "        \n",
    "        prev_memories = new_memories\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        t+=1\n",
    "\n",
    "subm_env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "gym.upload(save_path,\n",
    "           \n",
    "           #this notebook\n",
    "           writeup=<url to my gist>, \n",
    "           \n",
    "           #your api key\n",
    "           api_key=<my_own_api_key>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Once you got it working,\n",
    "Try building a network that maximizes the final score\n",
    "\n",
    "* Moar lasagne stuff: convolutional layers, batch normalization, nonlinearities and so on\n",
    "* Recurrent agent memory layers, GRUMemoryLayer, etc\n",
    "* Different reinforcement learning algorithm (p.e. qlearning_n_step), other parameters\n",
    "* Experience replay pool\n",
    "\n",
    "\n",
    "Look for examples? Try examples/Deep Kung Fu for most of these features\n",
    "\n",
    "\n",
    "You can also try to expand to a different game: \n",
    " * all OpenAI Atari games are already compatible, you only need to change GAME_TITLE\n",
    " * Other discrete action space environments are also accessible this way\n",
    " * For continuous action spaces, either discretize actions or use continuous RL algorithms (e.g. .learning.dpg_n_step)\n",
    " * Adapting to a custom non-OpenAI environment can be done with a simple wrapper\n",
    " \n",
    " \n",
    "__Good luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
